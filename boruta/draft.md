borutaの下書き

## まずは適当に書き出す
### 必要な知識
- ランダムフォレストが特徴量重要度を算出することを知ってる
- なんとなく検定を知っている

### 特徴量選択の必要性
- 特徴量選択(feature selection, 変数選択)は原因分析に使われる。
- kaggleでは判別の精度が重要視されるが、実務上どうしてそのような判別をしたのかが重要である。回帰問題でも同じである。
- 例えば製造業などの場合、欠陥品か否かを見分けるシステムを作るよりも、欠陥品そのものを減らせたほうが良い。そこで欠陥品となる原因を知ることもデータサイエンスに求められている。
- そこで用いられるのが特徴量選択である。

- 特徴量選択した結果、モデルの学習や推論が高速化されるメリットもある。また、判別の精度がよくなったりすることもある。


### borutaとはなにか
- ランダムフォレストと検定を用いた特徴量選択手法の一つ
- 経験上非常に強力
- 判別や回帰の性能が著しく下がったことはない。下がっても誤差の範囲内。
- 元論文
- python実装(ただしバグがあり、動かない。あとで修正したものを提示する。)

###  よく知られた手法との比較
- ランダムフォレスト
    - どれぐらいの特徴量重要度があったら重要だと言えるのかがイマイチ
    - ランダム性から訓練するたびに特徴量重要度が変動する

- Forward selectionやBackward eliminationと言ったステップワイズな方法
    - 選んだ特徴量が過学習する

- lasso
    - 選んだ特徴量が過学習する

### シンプルにアイデア
1. 判別に寄与しないはずの偽の特徴量を作ってランダムフォレストを訓練。
2. 各特徴量の重要度と偽の特徴量の特徴量を比較。
3. 複数回比較し検定を行うことで、本当に重要な特徴量のみを選択。

以下では判別(回帰)に少しでも寄与するという意味を込めて「重要」と呼ぶことにする。

### もと詳しく
#### 1. 判別に寄与しないはずの偽の特徴量を作ってランダムフォレストを訓練。
(shadow featuresの図を挿入)

1. もともとのDataFrame(Original data)をコピーする。これをShadow featuresと呼ぶことにする。
2. Shadow featuresの各列に対して、サンプルをシャッフルする。これで各特徴量は判別に寄与しないはずの特徴量になった。

このShadow featuresが偽の特徴量となる。
3. Original dataとShadow featuresを結合してランダムフォレストを訓練する入力とする。

#### 2. 各特徴量の重要度と偽の特徴量の重要度を比較。
1. ランダムフォレストを訓練したら、Original dataとShadow featuresの双方から特徴量の重要度を得ることができる。

2. Shadow featuresの中で一番大きな特徴量重要度を記録しておく。この操作によって、「寄与しないはずの特徴量でもこれぐらいの重要度になりえる」という目安になる。

3. ではこの「Shadow featuresの中で一番大きな重要度を持つ特徴量」よりも重要な特徴量が、真に重要だと言ってしまって良いのだろうか。ランダムフォレストの性質により、特徴量の重要度は訓練するたびに変動する。一回の訓練ではたまたま選ばれたり、たまたま選ばれなかったりする特徴量も出てきてしまう。


#### 3. 複数回比較し検定を行うことで、本当に重要な特徴量のみを選択。
そして検定へ。何回もランダムフォレストを訓練し、重要そうな特徴量を複数回記録する。そして各特徴量に対して重要かどうかの検定を行う。

重要そうな特徴量の記録については下図のように非常にシンプルである。
(例の図を挿入)
Shadow featuresの中で最大の重要度よりも大きな特徴量について、それが選ばれるたびhitを+1していく。例えばランダムフォレストを10回訓練し、8回とも選ばれるような特徴量が存在したら、その特徴量のhitには8が格納されている。

このhitをk, ランダムフォレストを訓練した回数をnとし、p=0.5の二項分布と対応する確率を用いて検定を行う。詳細は次章で述べる。

### 二項分布による検定
検定を行う際にここでは仮説を3つ用意する。

- 帰無仮説
    - この特徴量の重要度は、判別(回帰)に寄与しない特徴量の重要度と同じである。
- 対立仮説1
    - この特徴量の重要度は、判別(回帰)に寄与しない特徴量の重要度よりも大きい。
- 対立仮説2
    - この特徴量の重要度は、判別(回帰)に寄与しない特徴量の重要度よりも小さい。




## 流れ

