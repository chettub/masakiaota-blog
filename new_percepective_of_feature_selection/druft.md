## サーベイ論文まとめ
### 何を書こうか
- (特徴量同士の評価指標)(filterのときの説明の中に入れちゃおう)
- 特徴量選択の難しさ
- 特徴量選択の手法の外観図
- 教師あり特徴量選択（需要が大きそうだから）
  - filter
    - これに関しては説明できそう
  - wrapper
    - 代表的な手法であるSVM-RFEぐらいは解説したい
      - SVMとRecursive feature eliminationを組み合わせた手法である
      - 初めて使われたのがおそらく2002年(今では結構古い手法かも)
      - [x] ライブラリも紹介したい(sklearnでいっか)
    - 木とGAを用いた手法
    - Borutaも繰り返しモデルを構築するという点でwrapper methodである。
  - embedding
    - ridgeとか
    - 特に論文では言及されてなかったが、regularized treeを紹介してみよう
      - [x] ライブラリも紹介したい
      - [x] https://cran.r-project.org/web/packages/RRF/RRF.pdf

- 特徴量選択の新展開(別記事にする？)



# 特徴量選択の今と新展開

### 特徴量選択とは
特徴量選択(Feature Selection, 変数選択とも)はデータサイエンスにおいて非常に重要である。 
例えば、製造業において欠陥品を判別するタスクを考えてみよう。
このタスクは本当に欠陥品を判別するだけがゴールなのだろうか。
本当に知りたいのは欠陥品がどうして生じるか、という点だろう。
例えば、欠陥品と関係のあるセンサーや工程と提示できたら専門家たちはそこを改良し欠陥品をそもそも生み出さずに済むことができる。
これが本当のゴールである。

ここで重要なのが特徴量選択である。

また、良い特徴量選択は以下の恩恵が期待できる。

- 学習時間を減らせる
- モデルの解釈性が向上する
- モデルの精度が向上する
- 過学習を減らせる

以前に当ブログでも特徴量選択のまとめのような記事を書いた。

https://aotamasaki.hatenablog.com/entry/2018/04/18/201127

これはかなり初心者向けの内容になってしまったが今回は、もっと踏み込んだ内容にしようと思う。
具体的には、特徴量選択の手法の全体像を紹介してから、需要が大きいとされる教師ありの特徴量選択について代表的な手法をいくつか紹介する。
また、最後に最新の特徴量選択の動向も紹介する。

今回紹介する半分以上の内容は、こちらのサーベイ論文にあるので興味がある方はどうぞ。

https://www.researchgate.net/publication/323661651_Feature_selection_in_machine_learning_A_new_perspective


本記事の想定読者は大学学部レベルの機械学習、確率統計、情報理論をかじったことのある方であるが、初学者にも伝わる部分があるように努力した。

#### 特徴量選択の難しさ
- 例えば、100特徴量から最適な特徴量の部分集合を見つけろっという問題は、2^100 - 1通り試すことになる。
- 1通りあたり1秒で学習が終わるとしても、全通り試すには宇宙誕生から今までをあと2^41(2200億)回繰り返さなければいけない計算になる（宇宙の年齢を2^59秒ほどとした）
- これは典型的な組み合わせ最適化問題である。

#### 特徴量選択の手法の大別
ここでは特徴量選択の手法を9種に分類する。これは以下の2つの事項に注目した結果である。
- 教師データの有無(3種)
- 特徴量を選択するのにどんな方法を使うか(3種)

まず機械学習と同様に教師データの有無で三種に分類できる。
- 教師データの有無(3種)
  - supervised(教師ありの特徴量選択)
  - unsupervised(教師なしの特徴量選択)
  - semi-supervised(半教師ありの特徴量選択)

以上のそれぞれについて、さらに3つの戦略が考えられる。
- 特徴量を選択するのにどんな方法を使うか(3種)
  - filter method
  - wrapper method
  - embedding method

これらがどういうものかについては本記事で述べるので仮に知らなくても構わない。

大きくは以上の分類で目的の特徴量選択方法を探すことができると思うが、場合によってはいかに示すような分類方法もある。

- 特徴量選択の評価指標はなにか
  - filter methodの場合これも考慮したほうがよいだろう
  - 相関係数、ユークリッド距離、エントロピー、情報量基準等のなにを用いて特徴量選択をしているのかということ
- 特徴量の部分集合の探索戦略はなにか
  - wrapper methodの場合これも考慮したほうがよいだろう
  - 一つずつ特徴量を追加していくのか、削除していくのか、それとも他のやりかたで探索していくか
  - 具体例も踏まえながら本記事で後述する
- 出力の形式はどうか
  - ランキング形式と部分集合形式がある。
  - 目的によっては重要な分類である。


本記事では、需要が大きそうな教師ありの特徴量選択について見ていく。


### 教師ありの特徴量選択
ここでは教師ありの分類問題を仮定する。

#### filter method
- filter methodとは

##### 関係性を上げて冗長性を下げる

JMI https://github.com/jundongl/scikit-feature/blob/master/skfeature/example/test_JMI.py

mRMR
https://github.com/fbrundu/pymrmr


##### 関係性を上げて多様性を上げる
- しかし実装があまりなく、私が調べた限りでは見つからなかった。ご存知の方がいたら教えていただきたい。



#### wrapper method
featureのsearch sterategyが重要



ベースライン SVM-RFE
scikit-learnで実装できるので略

以前、borutaというのを紹介した。
これは判別性能は見ていないが、複数回モデルを構築して特徴量を選ぶという意味でwrapper method


xgboostを用いたborutaの親戚
https://github.com/chasedehan/BoostARoota

ただ、これらは冗長なものも取り込んでしまうことに注意。


#### embedding method
- モデルを学習すると同時に、使う特徴量も自動的に決定してしまおうという枠組みである。
- 一番ナイーブな方法はRidge(L1正則化)をモデルに組み込むこと。(but基本的に線形モデルになってしまう
- 非線形なモデルに対応したものとして、regularized treeを採用したランダムフォレスト(RRF)が挙げられる。(これのアイデアはシンプルで木を成長させるときに余分な特徴量を取り込まないようにするというアイデアである)
- https://cran.r-project.org/web/packages/RRF/RRF.pdf
- https://github.com/softwaredeng/RRF




### 特徴量選択の新展開

#### 超高次元データの特徴量選択

#### インバランスデータの特徴量選択

#### 特徴量選択のアンサンブル

#### オンライン特徴量選択

#### 深層学習を用いた特徴量選択




